{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuchihl/intro_ml_uci/blob/main/day_1/intro_to_ML_day_1_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHyH5eFeJAS"
      },
      "source": [
        "# 1. Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Configure plotting\n",
        "rc('animation', html='jshtml')\n",
        "qualitative_colors = ['#1b9e77','#d95f02','#7570b3','#e7298a']\n",
        "\n",
        "# Set the random seed\n",
        "rng = np.random.RandomState(1)"
      ],
      "metadata": {
        "id": "KNCuuwTEvrKt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlsfScOT4_4t"
      },
      "source": [
        "## 1.1 Load Diabetes Data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Throughout, we'll be using scikit-learn (sklearn), a popular open-source Python library for data analytics. It includes a wide range of algorithms for classification, regression, clustering, and dimensionality reduction."
      ],
      "metadata": {
        "id": "4GFCPHJYJ5zl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l08xjoTfYViZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRPwZfz8YXTT"
      },
      "outputs": [],
      "source": [
        "# Make the data set.\n",
        "diabetes_bunch = load_diabetes()\n",
        "\n",
        "print(diabetes_bunch.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sh32Dl4_n8K"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "X = diabetes_bunch.data\n",
        "y = diabetes_bunch.target\n",
        "\n",
        "# Standardize the targets:\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "# Use only one feature: body mass index (BMI), standardized\n",
        "X = X[:, np.newaxis, 2]\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training/testing sets\n",
        "X_train = X[:-20]\n",
        "\n",
        "X_test = X[-20:]\n",
        "\n",
        "# Split the targets into training/testing sets\n",
        "y_train = y[:-20]\n",
        "y_test = y[-20:]\n",
        "\n",
        "print(f\"X_train: {X_train.shape}\\nX_test:  {X_test.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\\ny_test:  {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c34XfB1dYeOQ"
      },
      "outputs": [],
      "source": [
        "# Plot the dataset\n",
        "fig, axs = plt.subplots(figsize=(3.,3.), nrows=1, ncols=1, facecolor='white', dpi=100)\n",
        "axs.scatter(X_train, y_train, color=qualitative_colors[0], s=1)\n",
        "axs.set_xlabel('BMI (standardized)')\n",
        "axs.set_ylabel('Disease Progression')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGKO7-SeYogp"
      },
      "source": [
        "## 1.2 Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0e9vwetcSSc"
      },
      "source": [
        "Given $n$ training points of the form $\\{(x_i,y_i)\\}_{i=1}^N$, where each $x_i$ is an **input** quantity and each $y_i$ is a **truth target**, or goal is to find a linear trend that best describes the data.\n",
        "\n",
        "**Model**: A straight line $y = w_1 x + w_0$, where $w_0$ and $w_1$ are coefficients we aim to learn.\n",
        "\n",
        "We use the letter $w$ because we think of the coefficients as **weights**; you should think about $y$ as a function of $x$ given the parameters $(w_0, w_1)$, i.e. $y=y(x|w_0,w_1)$.\n",
        "\n",
        "**Linear Regression**: find the settings for $(w_0,w_1)$ that best fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwO51_HPZ9wg"
      },
      "source": [
        "How do we quantify best fit? We have to specify a **loss function** that tells us how well we matched our data. Our goal is to minimize the loss, closing the gap between the model's predictions and the truth targets.\n",
        "\n",
        "One common loss function is the **squared-error** loss function, which quantifies the distance between the model's predictions $\\widehat{y}_i = w_1x_i + w_0$ and the truth target $y_i$ as\n",
        "\n",
        "\\begin{align}\n",
        "L(w_0, w_1)\n",
        "  &= \\sum_{i=1}^N (y_i - \\widehat{y}_i)^2 \\\\\n",
        "  &= \\sum_{i=1}^{N} (y_i - (w_1 x_i + w_0))^2\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvvTnsYVZ_Ql"
      },
      "outputs": [],
      "source": [
        "def squarederror_loss(xs, ys, w0, w1):\n",
        "  \"\"\"Caculate the squared-error loss\n",
        "    Parameters\n",
        "    ----------\n",
        "    xs : array_like\n",
        "        x-axis values of data points, shape (number of data points)\n",
        "    ys : array_like\n",
        "        y-axis values of data points, shape (number of data points)\n",
        "    w0 : array_like\n",
        "        weight for intercept, shape (number of weights)\n",
        "    w1 : array_like\n",
        "        weight for slope, shape (number of weights)\n",
        "    Returns\n",
        "    -----------\n",
        "    loss : array_like\n",
        "        squared-error loss, shape (number of weights)\n",
        "    \"\"\"\n",
        "  xs = np.asarray(xs).flatten()\n",
        "  ys = np.asarray(ys).flatten()\n",
        "  loss = np.sum((ys[:,np.newaxis] - (w1[np.newaxis,:] * xs[:,np.newaxis] + w0[np.newaxis,:]))**2.,axis=0)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6RrHTrKZ_3G"
      },
      "source": [
        "The squared-error loss function is minimized when the partial derivatives with respect to $ w_0 $ and $w_1 $ are zero:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial w_0} \\sum_{i=1}^{N} (y_i - (w_1 x_i + w_0))^2 = 0 $$\n",
        "\n",
        "$$ \\frac{\\partial}{\\partial w_1} \\sum_{i=1}^{N} (y_i - (w_1 x_i + w_0))^2 = 0$$\n",
        "\n",
        "These conditions produce for us the values of $w_0$ and $w_1$ that minimize the squared error loss,\n",
        "\n",
        "\\begin{align}\n",
        "w_0 &= \\frac{1}{N}\\sum_{i=1}^N \\big(y_i - w_1 x_i\\big) \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "w_1 &= \\frac{\\frac{1}{N} (\\sum_{i=1}^N x_i y_i) - \\bar{x} \\bar{y}}{\\frac{1}{N}(\\sum_{i=1}^N x^2_i) - \\bar{x}^2}\n",
        "\\end{align}\n",
        "\n",
        "where here $\\bar{x}$ and $\\bar{y}$ are the averages of the $x_i$'s and $y_i$'s respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv4OHTXcbcB8"
      },
      "outputs": [],
      "source": [
        "def univariate_linear_regression(xs, ys):\n",
        "  \"\"\"Calculate optimal weights in the univariate linear regression case.\n",
        "    Parameters\n",
        "    ----------\n",
        "    xs : array_like\n",
        "        x-axis values of data points, shape (number of data points)\n",
        "    ys : array_like\n",
        "        y-axis values of data points, shape (number of data points)\n",
        "    Returns\n",
        "    -----------\n",
        "    w0 : float\n",
        "        weight for intercept\n",
        "    w1 : float\n",
        "        weight for slope\n",
        "    \"\"\"\n",
        "  xs = np.asarray(xs).flatten()\n",
        "  ys = np.asarray(ys).flatten()\n",
        "  N = float(xs.shape[0])\n",
        "  w1 = (np.sum(xs*ys)/N - np.mean(xs)*np.mean(ys) ) / ( np.sum(xs**2.)/N - np.mean(xs)**2.)\n",
        "  w0 = (np.mean(ys) - w1*np.mean(xs))\n",
        "  return w0, w1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfgPXggTecbg"
      },
      "outputs": [],
      "source": [
        "# Calculate our analytic solutions\n",
        "w0, w1 = univariate_linear_regression(X_train, y_train)\n",
        "print(w0, w1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeEzVs7AelWv"
      },
      "outputs": [],
      "source": [
        "# Plot the data set with the linear regression fit\n",
        "plot_xs = np.linspace(X_train.min(), X_train.max(), 101)\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=100)\n",
        "axs.scatter(X_train, y_train, s=1, color=qualitative_colors[0])\n",
        "axs.plot(plot_xs, w1*plot_xs + w0, color=qualitative_colors[1], linewidth=2)\n",
        "axs.set_xlabel('BMI (standardized)')\n",
        "axs.set_ylabel('Diabetes Progression')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZRNB12c7zlU"
      },
      "source": [
        "#### So, we've found the values of $(w_0, w_1)$ that minimize the loss function. How bad are other values? Let's make a grid of $(w_0, w_1)$ values, and then calculate the loss function at each point. This is typically called a **loss landscape**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGvaHZIzfJJl"
      },
      "outputs": [],
      "source": [
        "# Define a range of w0 and w1 values to calculate.\n",
        "dw0, dw1 = 2, 2\n",
        "w0_range = np.linspace(w0 - dw0, w0 + dw0, 200) # vary around 0.01\n",
        "w1_range = np.linspace(w1 - dw1, w1 + dw1, 200) # vary around 12\n",
        "\n",
        "# Make a 200 x 200 grids of w0 and w1 values over that range.\n",
        "XX, YY = np.meshgrid(w0_range, w1_range)\n",
        "\n",
        "# Flatten the grids to lists with 40000 values.\n",
        "XY = np.c_[XX.ravel(), YY.ravel()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iUDRt0Ghjej"
      },
      "outputs": [],
      "source": [
        "# Calculate the squared-error loss for each pair of w0, w1 values.\n",
        "Z = squarederror_loss(X_train, y_train, XY[:,0], XY[:,1])\n",
        "\n",
        "# Turn the list of loss values into a 200 x 200 grid.\n",
        "Z = Z.reshape(XX.shape)\n",
        "\n",
        "# Also calculate the loss for the optimal w0, w1 values we found earlier.\n",
        "best_fit_loss = squarederror_loss(X_train, y_train, np.array([w0]), np.array([w1]))[0]\n",
        "print(f\"Total Squared Error: {best_fit_loss:.4f}\")\n",
        "print(f\"Mean Squared Error: {(best_fit_loss / len(X_train)):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDbg5iU1i2AK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss Landscape Visualization\n",
        "plt.figure(figsize=(6,5), dpi=100)\n",
        "\n",
        "# Plot the loss surface as filled contours\n",
        "contour = plt.contourf(\n",
        "    XX, YY, Z,\n",
        "    levels=50,\n",
        "    cmap=\"viridis\",\n",
        "    alpha=0.85\n",
        ")\n",
        "cbar = plt.colorbar(contour, label=\"Squared Error Loss\")\n",
        "\n",
        "# Overlay contour lines\n",
        "plt.contour(XX, YY, Z, levels=10, colors=\"k\", alpha=0.4, linewidths=0.5)\n",
        "\n",
        "# Mark the best-fit point\n",
        "plt.scatter(w0, w1, color=\"red\", edgecolor=\"white\", s=60, label=\"Best fit (min loss)\")\n",
        "\n",
        "plt.title(\"Loss Landscape over (w_0, w_1)\")\n",
        "plt.xlabel(\"Intercept w_0\")\n",
        "plt.ylabel(\"Slope w_1\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWYEQpbTlsz6"
      },
      "source": [
        "Clearly, the loss landscape has one unique minimum, the point we found via partial derivatives.\n",
        "\n",
        "What if we had a more complicated loss landscape, so that we couldn't take partial derivatives?\n",
        "\n",
        "This motivates our study of **gradient descent**, an iterative algorithm designed to search through a loss landscape for minima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtdVf8q59bvq"
      },
      "source": [
        "## 1.3 Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIzpihvv9z-I"
      },
      "source": [
        "### Initialization\n",
        "\n",
        "Our goal is to find our way to the settings of $(w_0,w_1)$ that minimize the loss function - the basin in the plot above.\n",
        "\n",
        "We need to choose a starting point in the weight space - an initial guess for $w_0$ and $w_1$. This is called **initializing** the weights of the model.\n",
        "\n",
        "A simple method to do this is to randomly choose a point, but this could be very far from the optimal position.\n",
        "\n",
        "In linear regression, you'll have some intuition about your data, and can effectively eyeball where to start. In more complicated machine learning algorithms, initialization is more subtle, yet crucial to achieving stable training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqjeu9wK-Odi"
      },
      "outputs": [],
      "source": [
        "w0_init, w1_init = -0.2, 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqVG_K4V-R80"
      },
      "source": [
        "### Gradients\n",
        "\n",
        "We've picked a starting point in our loss landscape. The goal is now to follow the **gradient** of the loss function, that is, the direction of steepest change. Since our goal is to find the loss minimum, we must climb against the gradient; hence, gradient descent.\n",
        "\n",
        "The gradient at initialization has two components, one for each weight:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_0} \\sum_{i=1}^{n} (y_i - (w_1 x_i + w_0))^2 &= -2 \\sum_{i=1}^{n}(y_i - (w_1 x_i + w_0)) \\\\\n",
        "\\frac{\\partial}{\\partial w_1} \\sum_{i=1}^{n} (y_i - (w_1 x_i + w_0))^2 &= -2 \\sum_{i=1}^{n}(y_i - (w_1 x_i + w_0)) x_i\n",
        "\\end{align}\n",
        "\n",
        "Remember here the value of the gradient assumes the initial values of $(w_1, w_0)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE5HndSl_Ofl"
      },
      "outputs": [],
      "source": [
        "def gradient_of_weights(xs, ys, w0, w1):\n",
        "  \"\"\"Calculate the partial derivative of the loss function with respect to the weights.\n",
        "  Parameters\n",
        "  ----------\n",
        "  xs : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  ys : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0 : float\n",
        "    weight for intercept\n",
        "  w1 : float\n",
        "      weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  derivative_w0 : float\n",
        "      partial derivative of the loss function with respect to theweight for slope\n",
        "  derivative_w1 : float\n",
        "      partial derivative of the loss function with respect to the weight for slope\n",
        "  \"\"\"\n",
        "\n",
        "  derivative_w0 = np.sum((ys - (w1*xs + w0)) )\n",
        "  derivative_w1 = np.sum((ys - (w1_init*xs + w0_init))*xs)\n",
        "\n",
        "  return derivative_w0, derivative_w1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WmB_KN5AlaI"
      },
      "source": [
        "### Descent\n",
        "\n",
        "#### Situated at our initial guess, we've got a gradient. We now need to take a step in the direction of the (negative) gradient. How big should that step be? Well, this turns out to be extremely important. In machine learning, the gradient step size is called the **learning rate**, and tuning the learning rate is absolutely crucial to training a performant model.\n",
        "\n",
        "#### Let's call our learning rate $\\eta$. Then, we can take a step of size $\\eta$ in the direction of the gradient, to update the values of our weights:\n",
        "\n",
        "\\begin{align}\n",
        "w_0 &← w_0 + \\eta \\sum_{i=1}^{n}(y_j - (w_1 x_i + w_0))\n",
        "\\\\\n",
        "w_1 &← w_1 + \\eta \\sum_{i=1}^{n}(y_j - (w_1 x_i + w_0)) x_i\n",
        "\\end{align}\n",
        "\n",
        "To be clear, here's a nicely-packaged version of what we've done:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla_{\\mathbf{w}}L\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf{w}:=(w_0, w_1)$ and $\\nabla_\\mathbf{w} L = (\\frac{\\partial L}{\\partial w_0}, \\frac{\\partial L}{ \\partial w_1})$. This is the gradient descent weight update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABHuD3blBlwp"
      },
      "outputs": [],
      "source": [
        "def update_weights(xs, ys, w0_old, w1_old, lr=1e-4):\n",
        "  \"\"\" Update the weights using the partial derivate and step size.\n",
        "  Parameters\n",
        "  ----------\n",
        "  xs : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  ys : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0_old : float\n",
        "    weight for intercept\n",
        "  w1_old : float\n",
        "      weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  w0_new: float\n",
        "    weight for intercept\n",
        "  w1_new : float\n",
        "    weight for slope\n",
        "  \"\"\"\n",
        "  xs = np.asarray(xs).flatten()\n",
        "  ys = np.asarray(ys).flatten()\n",
        "  w0_new = w0_old + lr*np.sum((ys - (w1_old*xs + w0_old)) )\n",
        "  w1_new = w1_old + lr*np.sum((ys - (w1_old*xs + w0_old))*xs)\n",
        "  return w0_new, w1_new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Analytic Minimum:    w0={w0:.3f}, w1={w1:.3f}\")\n",
        "print(f\"Initialization:      w0={w0_init:.3f}, w1={w1_init:.3f}\")\n",
        "\n",
        "\n",
        "lr = 1e-4\n",
        "w0_first, w1_first = update_weights(X_train, y_train, w0_init, w1_init, lr=lr)\n",
        "print(f\"After First GD Step: w0={w0_first:.3f}, w1={w1_first:.3f}\")"
      ],
      "metadata": {
        "id": "hXRZyfZymA6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 1**: What happens to the first update as you vary learning rate?"
      ],
      "metadata": {
        "id": "e4lbLx9JPxZ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvorriZFCVMh"
      },
      "source": [
        "### Iteration\n",
        "\n",
        "We need to just keep repeating this process until we're satisfied that our updated values $(w_0, w_1)$ have converged. We'll define convergence to be when the changes to our weights are sufficiently small, i.e. $\\Delta w_0 = -\\eta\\times\\partial L/\\partial w_0< \\delta_0$ and $\\Delta w_1 = -\\eta\\times\\partial L/\\partial w_1 < \\delta_1$ for some fixed thresholds $\\delta_0,\\delta_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-NqtcDFkZ5p"
      },
      "outputs": [],
      "source": [
        "def univariate_gradient_decent(xs, ys, w0_init, w1_init, lr = 0.001):\n",
        "  \"\"\" Calculate the optimal weights using gradient descent.\n",
        "  Parameters\n",
        "  ----------\n",
        "  xs : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  ys : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0_init : float\n",
        "    initial guess of weight for intercept\n",
        "  w1_init : float\n",
        "    initial guess of weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  w0s: array_like\n",
        "    list of weights for intercept\n",
        "  w1s : array_like\n",
        "    list of weights for slope\n",
        "  \"\"\"\n",
        "  w0_firststep, w1_firststep = update_weights(xs, ys, w0_init, w1_init, lr=lr)\n",
        "\n",
        "  w0s  = np.array( [w0_init, w0_firststep] )\n",
        "  w1s  = np.array( [w1_init, w1_firststep] )\n",
        "\n",
        "  while (np.abs(w0s[-2]-w0s[-1]) > 1e-8) or (np.abs(w1s[-2]-w1s[-1]) > 1e-8):\n",
        "    w0_nextstep, w1_nextstep = update_weights(xs, ys, w0s[-1], w1s[-1], lr=lr)\n",
        "    w0s = np.append(w0s, w0_nextstep)\n",
        "    w1s = np.append(w1s, w1_nextstep)\n",
        "  return w0s, w1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEvHSjHRrTnv"
      },
      "outputs": [],
      "source": [
        "weights_to_plot0, weights_to_plot1 = univariate_gradient_decent(\n",
        "    X_train, y_train, w0_init, w1_init, lr=10e-4\n",
        ")\n",
        "steps_0 = weights_to_plot0[1:] - weights_to_plot0[:-1]\n",
        "steps_1 = weights_to_plot1[1:] - weights_to_plot1[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_to_plot0.shape"
      ],
      "metadata": {
        "id": "eYx7OlT2n9xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUYTyh2arWn7"
      },
      "outputs": [],
      "source": [
        "# Plot the squared-error loss values.\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=100, sharex=True)\n",
        "CS = axs.contour(XX, YY, Z)\n",
        "axs.clabel(CS, inline=True, fontsize=10, fmt='%d')\n",
        "axs.plot(weights_to_plot0, weights_to_plot1, '-', color='k')\n",
        "axs.scatter(w0_init, w1_init, s=10, color='red')\n",
        "axs.scatter(w0, w1, s=10, color='black')\n",
        "axs.set_ylabel(r'$w_1$')\n",
        "axs.set_xlabel(r'$w_0$')\n",
        "axs.set_xlim(w0-dw0, w0+dw0)\n",
        "axs.set_ylim(w1-dw1, w1+dw1)\n",
        "axs.set_title(\"Loss Contours\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Fitting Multiple Features"
      ],
      "metadata": {
        "id": "LQW9UD8DrCV8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKJbfPQvb4fc"
      },
      "source": [
        "Above is a very simple example, in just two dimensions, but it can extend to many dimensions. Let's fit the full dataset! We'll have to upgrade our notation a bit:\n",
        "\n",
        "Our dataset is now $D := \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$ for input vectors $\\mathbf{x}_i\\in\\mathbb{R}^{10}$. Our model is now a function of the data, given 11 parameters - one slope for each of the 10 input fields, and one for the intercept of the line. Collecting the slope parameters into a weight vector $\\mathbf{w}:=(w_{10},w_9,...,w_1)$, our model is now\n",
        "\n",
        "$$y_i(x_i|\\mathbf{w}, w_0) = \\mathbf{w}^T\\mathbf{x}_i + w_0$$\n",
        "\n",
        "where we can think about $\\mathbf{w}$ as **weights** and $w_0$ as a **bias**. This terminology will pop up again when we study deep neural networks!\n",
        "\n",
        "Note that the loss function is unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_14ttgycQWv"
      },
      "outputs": [],
      "source": [
        "# Load the full diabetes dataset\n",
        "diabetes_dataset = load_diabetes(as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBw9q1bpeYob"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(diabetes_dataset.frame, corner=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIjhLfw4G6Gm"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = diabetes_dataset.data\n",
        "print(features.columns)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features = pd.DataFrame(scaler.fit_transform(features))\n",
        "features.columns = diabetes_dataset.feature_names\n",
        "print(f\"\\nInput Means:\\n{features.mean(0)},\\n\\nInput Stds:\\n{features.std(0)}\")\n",
        "\n",
        "targets = diabetes_dataset.target\n",
        "targets = (targets - targets.mean()) / targets.std()\n",
        "print(f\"\\nTargets:\\nMean={targets.mean():.4f}, Std={targets.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isMTsIkPb9uW"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-mKJgDVcAdY"
      },
      "outputs": [],
      "source": [
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(features, targets)\n",
        "print(features.columns)\n",
        "print(reg.coef_)\n",
        "print(reg.intercept_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtWGy9MqEquf"
      },
      "outputs": [],
      "source": [
        "reg = linear_model.SGDRegressor(loss='squared_error', max_iter=10000)\n",
        "reg.fit(features, targets)\n",
        "print(reg.coef_)\n",
        "print(reg.intercept_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "y_pred = linear_model.LinearRegression().fit(features, targets).predict(features)\n",
        "\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.scatter(targets, y_pred, alpha=0.7)\n",
        "lims = [targets.min(), targets.max()]\n",
        "plt.plot(lims, lims, 'r--', label='Ideal fit')\n",
        "plt.xlabel(\"Actual target\")\n",
        "plt.ylabel(\"Predicted target\")\n",
        "plt.title(\"Predicted vs. Actual Diabetes Progression\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nrL37H6ctQUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last point to make. When we study **linear** regression, we're talking a model that is linear with respect to its weights, not its inputs. For example, if we had a transformation $\\phi(\\mathbf{x})$, which may be some complicated (non-linear) function, we could still do linear regression like this:\n",
        "\n",
        "$$\n",
        "  y = \\mathbf{w}^T\\phi(\\mathbf{x}) + w_0.\n",
        "$$\n",
        "\n",
        "This means linear regression can capture nonlinear relationships in the data — as long as those nonlinearities are expressed in the features."
      ],
      "metadata": {
        "id": "aE3XF1-BoRCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 2**: Suppose we have a quadratic relationship between inputs $x_i$ and targets,\n",
        "$$ y_i = 3x_i^2 + x_i. $$ Clearly, we won't do well if our model is $\\widehat{y_i}=w_1x_i + w_0$. Can you think of another linear regression model that would do well?"
      ],
      "metadata": {
        "id": "jcEgi8jLp3mb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn4WD4IJeCac"
      },
      "source": [
        "# 2. Classification via Linear Decision Boundaries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at another canonical machine learning setting: classification! Our data takes the same form as it did for regression, i.e. vector inputs $\\mathbf{x}_i$ and truth targets $y_i$, in full $D=\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$. The only difference is that our truth targets are binary (0 or 1) classification scores. This changes the nature of our task; instead of regressing continuous values, our goal is to draw a boundary that separates datapoints belonging to different classes. In this way, we will have partitioned, or classified, them."
      ],
      "metadata": {
        "id": "TbglxE1IeT1Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuGcyzsxHy64"
      },
      "source": [
        "## 2.0 Load Some Example Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKVh1m2SvWgM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFQjaUlozXxM"
      },
      "outputs": [],
      "source": [
        "# Make a synthetic dataset\n",
        "X, labels = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1, class_sep=3.)\n",
        "X += 2 * rng.uniform(size=X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuSzGYAgzebb"
      },
      "outputs": [],
      "source": [
        "# Plot the dataset\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=100)\n",
        "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
        "axs.set_xlabel('$x_1$')\n",
        "axs.set_ylabel('$x_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNmZUDPcn28L"
      },
      "source": [
        "## 2.1 Decision Boundaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt7g8F9GcXJD"
      },
      "source": [
        "A **decision boundary** is a line (or a hyperplane in more than two dimensions) that separates our classes.\n",
        "\n",
        "Data that can be entirely separated by a line is called **linearly separable** data. What might this line look like? Well, the goal is to find a line relating $x_2$ and $x_1$, above which points of one class (say yellow points, class 1) belong, and below which points of the other class (say purple points, class 0) belong. We'll parametrize the line as\n",
        "\n",
        "$ y(x)  = w_1 x + w_0 $\n",
        "\n",
        "where as before, $w_0$ and $w_1$ are the parameters of the model, learnable weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZbPdNeIJ9a5"
      },
      "outputs": [],
      "source": [
        "# Plot the data and with an initial guess for a decision boundary\n",
        "plot_xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 101)\n",
        "\n",
        "w0_init, w1_init = -2., 2.\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=100)\n",
        "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
        "axs.plot(plot_xs, w1_init*plot_xs + w0_init, color='k', linewidth=2)\n",
        "axs.set_xlabel('$x_1$')\n",
        "axs.set_ylabel('$x_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUWEyDBpMAZr"
      },
      "source": [
        "\n",
        "Obviously, though our data is linearly separable, we have totally failed to separate it.\n",
        "\n",
        "Let's try to figure out how well we did. Our classification labels are generated by comparing our points to the decision boundary. This is done by checking the sign of the following quantity:\n",
        "\n",
        "$$z := x_2 - y(x_1) = x_2 - (w_1 x_1 + w_0)$$\n",
        "\n",
        "- Above the line, i.e. $z > 0$ --> label 1\n",
        "\n",
        "- Below the line, i.e. $z < 0$ --> label 0\n",
        "\n",
        "Given these criteria, we can now implement our predictions for the class of each point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ktUyEwLCL5"
      },
      "outputs": [],
      "source": [
        "def predict_class(x1s, x2s, w0, w1):\n",
        "  \"\"\" Predict the class of each point.\n",
        "  Parameters\n",
        "  ----------\n",
        "  x1s : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  x2s : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0 : float\n",
        "    weight for intercept\n",
        "  w1 : float\n",
        "    weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  prediction: array_like\n",
        "    list of predicted classes\n",
        "  \"\"\"\n",
        "  prediction = np.asarray((x2s - (w1*x1s + w0)) > 0, dtype=int)\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W4sU7HALn9b"
      },
      "outputs": [],
      "source": [
        "predictions = predict_class(X[:, 0], X[:, 1], w0_init, w1_init)\n",
        "print(predictions)\n",
        "print(labels)\n",
        "print(f\"{np.sum(predictions == labels)} correct out of {labels.shape[0]} total\")\n",
        "print(f\"{np.sum(predictions == labels) / labels.shape[0] * 100 :0.1f}% accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqyowdxHq79m"
      },
      "source": [
        "## 2.2 Classification with scikit-learn\n",
        "\n",
        "Okay, instead of randomly guessing, we should train a classifier. We'll use a scikit-learn classifier that is trained via gradient descent. The goal is the same: find settings for $w_0$ and $w_1$ that minimize our loss function, which in this case is the **logistic loss** (sometimes called **binary cross entropy**, or **BCE**).\n",
        "\n",
        "Before showing the explicit form of the loss function, recall that our deviation from the linear decision boundary was the quantity\n",
        "\n",
        "$$ z := x_2 - w_1x_1 - w_0 $$\n",
        "\n",
        "which is positive for class 1 and negative for class 0. We can squeeze this quantity into the range $[0,1]$, effectively producing a probability of $x_i$ corresponding to class 1, by applying the sigmoid function\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "so that when $z$ is increasingly positive, $\\sigma(z)$ approaches 1, and when $z$ is increasingly negative, $\\sigma(z)$ approaches 0."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 3**: Plot sigmoid as a function of $z$."
      ],
      "metadata": {
        "id": "ksoak1IXuZWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If we take our predictions to be probabilities $\\widehat{y}_i = \\sigma(z)$, we can compare them to the labels $y_i$ (either 0 or 1) with the BCE loss function\n",
        "\n",
        "$$ L(w_0, w_1) = -\\frac{1}{N}\\sum_{i=1}^N \\big(y_i\\log \\widehat{y}_i + (1-y_i)\\log(1-\\widehat{y}_i)\\big)$$\n",
        "\n",
        "Basically, we're looking at entropy scores that quantify the difference between our predicted probabilities and the target probabilities. The negative sign accounts for the fact that we're taking the log of probabilities, which are in the range $[0,1]$.\n",
        "\n",
        "\n",
        "**Note**: In practice, the model learns one weight per input feature, i.e. a hyperplane $w_2x_2 + w_1x_1 + w_0$. This is an equivalent (just a reparametrization) to what is discussed here."
      ],
      "metadata": {
        "id": "dHYd5nxcubLo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9NP1D4orGz2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVV1WiI_tDb2"
      },
      "outputs": [],
      "source": [
        "clf = SGDClassifier(loss='log_loss', max_iter=100)\n",
        "clf.fit(X, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEw_Z0z0rK6R"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=100)\n",
        "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
        "axs.plot(plot_xs, (-(plot_xs * clf.coef_[0, 0]) - clf.intercept_[0]) / clf.coef_[0, 1], color='k', linewidth=2)\n",
        "axs.set_xlabel('$x_1$')\n",
        "axs.set_ylabel('$x_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UugYZ-uNHv7L"
      },
      "source": [
        "# 3. Unsupervised Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous two methods we've examined have been **supervised**, in the sense that they look at data pairs $(x_i,y_i)$ with explicit training targets. What if you don't have access to training targets? This motivates the study of **unsupervised** methods. We'll look at several examples of **clustering** algorithms, that try to group similar datapoints without explicit labels."
      ],
      "metadata": {
        "id": "0T4FGc1ih3Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 The Breast Cancer Wisconsin Dataset\n",
        "\n",
        "This dataset contains cell nuclei measurements of breast masses. Each sample represents one patient’s biopsy, with features describing the shape and texture of cell nuclei (e.g. *radius, perimeter, smoothness, compactness,* *concavity*). The target variable has two values: malignant (0) or benign (1). We're going to ignore these targets when creating our model, and only use them to evaluate its performance."
      ],
      "metadata": {
        "id": "4pnstCt0JuVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer(as_frame=True)\n",
        "X = data.data\n",
        "y = pd.Series(data.target, name=\"label\")  # 0 = malignant, 1 = benign by default\n",
        "y = (y==0).astype(int) # reverse the labels for clarity\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_scaled.shape, y.value_counts()"
      ],
      "metadata": {
        "id": "aWogVeWJI37p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 K-Means Clustering\n",
        "\n",
        "Given $N$ data points $\\{\\mathbf{x}_i\\}_{i=1}^N$, our goal is to identify $K$ clusters (note: we have to specify $K$ up front). The algorithm is iterative with the following steps:\n",
        "\n",
        "0. **Randomly generate centroids** for each cluster.\n",
        "\n",
        "(Iterative steps)\n",
        "1. **Assign each point** to the nearest centroid.\n",
        "2. **Recompute** each centroid as the mean of all points assigned to it.\n",
        "3. **Repeat** until assignments stabilize (convergence).\n",
        "\n",
        "The algorithm (implicitly) minimizes the **within-cluster sum of squared distances** (the inertia). Given cluster centers $[\\boldsymbol{\\mu}_1, ..., \\boldsymbol{\\mu}_K]$, one vector location for each of the $K$ clusters, and cluster assignments $[z_1,...,z_N]$, one scalar for every data point, the inertia is\n",
        "\n",
        "$$J=\\sum_{i=1}^N ||\\mathbf{x}_i - \\boldsymbol{\\mu}_{z_i}||^2$$\n",
        "\n",
        "where:\n",
        "- $C_k$ = indices of points in cluster $k$,\n",
        "- $\\boldsymbol{\\mu}_k$ = mean (centroid) of those points.\n",
        "\n",
        "The below cell runs $K$-Means on our data; note that $K$-Means is sensitive to initialization, and we ask for 20 different runs (with different initial centroid placements) below. The one with the lowest inertia is chosen."
      ],
      "metadata": {
        "id": "op-ZAeDFK3gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, n_init=20, random_state=0)\n",
        "labels_km = kmeans.fit_predict(X_scaled)\n",
        "kmeans.inertia_"
      ],
      "metadata": {
        "id": "0pWCBSEcwR4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How well did we do? Well, it's kind of hard to visualize 30 dimensions. Instead, let's see if we can identify the 2 most relevant directions in this high dimensional space, so that we can plot them. This strategy is generically called **dimensionality reduction**"
      ],
      "metadata": {
        "id": "L0JtNLIWjqpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal Component Analysis (PCA)** is a technique that captures key aspects of a high-dimensional dataset in lower dimensions. PCA identifies **principal components** in the data, which are directions in feature space where the data varies the most (directions of maximum variance). It builds a set of orthogonal **principle axes** along these directions (here we're shooting for 2), and returns the projection of the data onto the principle axes."
      ],
      "metadata": {
        "id": "ca2DS4_XkDEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2, random_state=0)\n",
        "Z = pca.fit_transform(X_scaled)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(9,4), dpi=100)\n",
        "\n",
        "axes[0].scatter(Z[:,0], Z[:,1], c=y, cmap=\"coolwarm\", s=14, alpha=0.8)\n",
        "axes[0].set_title(\"True labels (Red=Malignant)\")\n",
        "axes[0].set_xlabel(\"PC1\"); axes[0].set_ylabel(\"PC2\"); axes[0].set_aspect(\"equal\")\n",
        "\n",
        "axes[1].scatter(Z[:,0], Z[:,1], c=labels_km, cmap=\"coolwarm\", s=14, alpha=0.8)\n",
        "axes[1].set_title(\"K-Means clusters (K=2)\")\n",
        "axes[1].set_xlabel(\"PC1\"); axes[1].set_ylabel(\"PC2\"); axes[1].set_aspect(\"equal\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZZdBDggFwHTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's next evaluate how we did using a **confusion matrix**, which summarizes how a classification (or clustering) model’s predictions compare to the true labels.\n",
        "\n",
        "|               | Predicted 0 | Predicted 1 |\n",
        "|----------------|-------------|-------------|\n",
        "| **True 0**     | True Negative (TN) | False Positive (FP) |\n",
        "| **True 1**     | False Negative (FN) | True Positive (TP) |\n",
        "\n",
        "From these four counts, we can compute key metrics:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}, \\quad\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}, \\quad\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "In this sense, precision tells us how many malignancies were in our pool of predicted malignancies, and recall tells us what fraction of malignancies were correctly identified from all biopsies."
      ],
      "metadata": {
        "id": "4ltnzLV_OIgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Compute classification metrics\n",
        "precision = precision_score(y, labels_km)\n",
        "recall = recall_score(y, labels_km)\n",
        "f1 = f1_score(y, labels_km)\n",
        "cm_km = confusion_matrix(y, labels_km, normalize=\"true\")\n",
        "\n",
        "# --- Print results ---\n",
        "print(f\"KMeans clustering evaluation:\")\n",
        "print(f\"  Precision: {precision:.3f}\")\n",
        "print(f\"  Recall:    {recall:.3f}\")\n",
        "print(f\"  F1 Score:  {f1:.3f}\")\n",
        "print(\"  Normalized confusion matrix (rows = true labels):\")\n",
        "print(np.round(cm_km, 3))\n"
      ],
      "metadata": {
        "id": "H-IfvRSwv--F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 DBSCAN\n",
        "\n",
        "There are many drawbacks to K-means clustering. First, it is undesirable to have to specify $K$ up front. Also, $K$-means assums the data can be split up into localized \"cells,\" which fails when the data is distributed as shown below."
      ],
      "metadata": {
        "id": "kZ6ykZF6ZDp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def make_counterexample():\n",
        "  # --- Generate disk (inner) and ring (outer shell) in 2D ---\n",
        "  rng = np.random.default_rng(0)\n",
        "\n",
        "  n_inner, n_shell = 300, 600\n",
        "  r_inner = 1.0\n",
        "  r_shell_in, r_shell_out = 1.5, 2.2\n",
        "\n",
        "  # Uniform in disk: r ~ sqrt(U)*R, theta ~ Uniform(0, 2π)\n",
        "  theta_i = rng.uniform(0, 2*np.pi, n_inner)\n",
        "  r_i = r_inner * np.sqrt(rng.random(n_inner))\n",
        "  Xi = np.c_[r_i * np.cos(theta_i), r_i * np.sin(theta_i)]\n",
        "\n",
        "  # Uniform in annulus: r ~ sqrt(U*(R2^2 - R1^2) + R1^2)\n",
        "  theta_o = rng.uniform(0, 2*np.pi, n_shell)\n",
        "  u = rng.random(n_shell)\n",
        "  r_o = np.sqrt(u*(r_shell_out**2 - r_shell_in**2) + r_shell_in**2)\n",
        "  Xo = np.c_[r_o * np.cos(theta_o), r_o * np.sin(theta_o)]\n",
        "\n",
        "  X = np.vstack([Xi, Xo])\n",
        "  y = np.r_[np.zeros(n_inner, dtype=int), np.ones(n_shell, dtype=int)]  # 0=disk, 1=ring\n",
        "\n",
        "  # --- K-Means with K=2 (shows the failure on non-convex clusters) ---\n",
        "  kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)\n",
        "  labels = kmeans.fit_predict(X)\n",
        "  centers = kmeans.cluster_centers_\n",
        "\n",
        "  # --- Plot: ground truth vs K-Means assignment ---\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(8, 4), dpi=100)\n",
        "  axes[0].scatter(X[:,0], X[:,1], c=y, s=12, alpha=0.8, cmap=\"viridis\")\n",
        "  axes[0].set_title(\"Ground truth: disk (0) vs ring (1)\")\n",
        "  axes[0].set_aspect(\"equal\")\n",
        "  axes[0].axis(\"off\")\n",
        "\n",
        "  axes[1].scatter(X[:,0], X[:,1], c=labels, s=12, alpha=0.8, cmap=\"viridis\")\n",
        "  axes[1].scatter(centers[:,0], centers[:,1], marker=\"X\", s=140, c=\"red\", edgecolor=\"k\", label=\"Centroids\")\n",
        "  axes[1].set_title(\"K-Means (K=2): splits by proximity\")\n",
        "  axes[1].set_aspect(\"equal\")\n",
        "  axes[1].axis(\"off\")\n",
        "  axes[1].legend(loc=\"lower left\", frameon=False)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "make_counterexample()"
      ],
      "metadata": {
        "id": "8xljcx1PLA9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike K-Means, which assumes roughly spherical clusters and requires choosing $K$ ahead of time,\n",
        "**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** finds clusters\n",
        "based on *regions of high density*.\n",
        "\n",
        "It builds clusters iteratively based on two parameters:\n",
        "- **ε (epsilon):** the neighborhood radius around each point\n",
        "- **min_samples:** the minimum number of neighbors required to form a dense region\n",
        "\n",
        "Algorithm outline:\n",
        "1. Pick an unvisited point.\n",
        "2. If it has at least `min_samples` points within distance `ε`, start a new cluster.\n",
        "3. Recursively include all density-connected points.\n",
        "4. Points not belonging to any cluster are labeled as **noise** (`-1`).\n",
        "\n",
        "DBSCAN can find arbitrarily shaped clusters and automatically ignores outliers,\n",
        "making it powerful for physical-science data where signal and noise differ by density."
      ],
      "metadata": {
        "id": "vLxWHbo0awUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "\n",
        "# --- helper: generate \"galaxy clusters over dense background of stars\" ---\n",
        "def make_galaxy_clusters(random_state=0):\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    # three compact Gaussian \"galaxy\" clusters\n",
        "    c1 = rng.normal([0.0, 0.0], [0.15, 0.15], size=(500, 2))\n",
        "    c2 = rng.normal([2.0, 1.0], [0.20, 0.12], size=(450, 2))\n",
        "    c3 = rng.normal([-1.7, 1.8], [0.18, 0.18], size=(400, 2))\n",
        "    # dense uniform background \"field stars\"\n",
        "    bg = rng.uniform(-3.0, 3.0, size=(800, 2))\n",
        "    X = np.vstack([c1, c2, c3, bg])\n",
        "    return X\n",
        "\n",
        "# --- build both datasets ---\n",
        "X_moons, _ = make_moons(n_samples=800, noise=0.05, random_state=42)\n",
        "X_galaxy = make_galaxy_clusters(random_state=0)\n",
        "\n",
        "# --- choose DBSCAN params per dataset (tuned for these scales) ---\n",
        "db_params = {\n",
        "    \"moons\": dict(eps=0.2, min_samples=5),\n",
        "    \"galaxy\": dict(eps=0.15, min_samples=10),\n",
        "}\n",
        "\n",
        "# --- fit clusterers ---\n",
        "km_moons = KMeans(n_clusters=2, n_init=10, random_state=0).fit(X_moons)\n",
        "db_moons = DBSCAN(**db_params[\"moons\"]).fit(X_moons)\n",
        "\n",
        "km_gal = KMeans(n_clusters=4, n_init=10, random_state=0).fit(X_galaxy)   # KMeans guesses a K; 4 works reasonably here\n",
        "db_gal = DBSCAN(**db_params[\"galaxy\"]).fit(X_galaxy)\n",
        "\n",
        "# --- plot: 2 rows (Moons, Galaxy), 2 cols (KMeans, DBSCAN) ---\n",
        "fig, ax = plt.subplots(2, 2, figsize=(9, 8), dpi=100)\n",
        "\n",
        "# Row 1: Two Moons\n",
        "ax[0, 0].scatter(X_moons[:, 0], X_moons[:, 1], c=km_moons.labels_, cmap=\"tab10\", s=12)\n",
        "ax[0, 0].set_title(\"K-Means (Two Moons)\")\n",
        "ax[0, 1].scatter(X_moons[:, 0], X_moons[:, 1], c=db_moons.labels_, cmap=\"tab10\", s=12)\n",
        "ax[0, 1].set_title(\"DBSCAN (Two Moons)\")\n",
        "\n",
        "# Row 2: Galaxy clusters + dense background\n",
        "ax[1, 0].scatter(X_galaxy[:, 0], X_galaxy[:, 1], c=km_gal.labels_, cmap=\"tab10\", s=8)\n",
        "ax[1, 0].set_title(\"K-Means (Galaxy clusters + background)\")\n",
        "# Color noise (-1) as light gray for clarity\n",
        "labels = db_gal.labels_.copy()\n",
        "colors = np.where(labels == -1, -1, labels)\n",
        "mask = labels >= 0\n",
        "ax[1, 1].scatter(X_galaxy[:, 0][mask], X_galaxy[:, 1][mask],\n",
        "                 c=labels[mask], cmap=\"tab10\", s=8)\n",
        "# Overlay noise points explicitly\n",
        "ax[1, 1].scatter(X_galaxy[labels==-1, 0], X_galaxy[labels==-1, 1],\n",
        "                 c=\"lightgray\", s=6, label=\"Noise\")\n",
        "ax[1, 1].set_title(\"DBSCAN (Galaxy clusters + background)\")\n",
        "ax[1, 1].legend(loc=\"lower left\", fontsize=8, frameon=False)\n",
        "\n",
        "# aesthetics\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax[i, j].set_aspect(\"equal\")\n",
        "        ax[i, j].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Notes:"
      ],
      "metadata": {
        "id": "3UiwWQELa5ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 4**: What happens if you change `eps` and `min_samples` above? Try out your strategy on `X_galaxy`."
      ],
      "metadata": {
        "id": "NUwzal_p2sdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solutions to Exercises"
      ],
      "metadata": {
        "id": "v86NFkFcQA3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        " You should notice that only small-ish values of the learning rate produce reasonably-sized updates to w_0. As lr grows, you jump over the minimum!"
      ],
      "metadata": {
        "id": "I8X7eSIvQHkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "\n",
        "Just take $\\widehat{y_i} = w_2 x_i^2 + w_1 x_i + w_0$, where we simply treat $x_i$ and $x_i^2$ as two separate inputs."
      ],
      "metadata": {
        "id": "4-K1QK6drZYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "\n",
        "```\n",
        "z = np.linspace(-10, 10, 1000)\n",
        "sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
        "fig = plt.figure(figsize=(3,3), dpi=100)\n",
        "plt.plot(z, sigmoid(z), 'b-')\n",
        "plt.xlabel(r\"$z$\")\n",
        "plt.ylabel(r\"$\\sigma(z)$\")\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "zRwQ41Kru7gX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "```\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def kth_neighbor_distances(X, k, metric=\"euclidean\", algorithm=\"auto\"):\n",
        "    \"\"\"\n",
        "    Return the sorted distances to each point's k-th nearest neighbor.\n",
        "    Note: we ask for k+1 neighbors because the closest neighbor is the point itself (distance 0).\n",
        "    \"\"\"\n",
        "    nn = NearestNeighbors(n_neighbors=k+1, metric=metric, algorithm=algorithm)\n",
        "    nn.fit(X)\n",
        "    dists, _ = nn.kneighbors(X)\n",
        "    kth = dists[:, k]              # distance to the k-th neighbor (excluding self)\n",
        "    return np.sort(kth)            # sort for elbow visualization\n",
        "\n",
        "k = 10  # this is our guess for min_samples\n",
        "kth_sorted = kth_neighbor_distances(X_galaxy, k=k)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(kth_sorted)), kth_sorted, lw=2)\n",
        "plt.xlabel(\"Points sorted by k-distance\")\n",
        "plt.ylabel(f\"Distance to {k}-th nearest neighbor\")\n",
        "plt.title(f\"k-distance plot (k={k})\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "1xnHdLJX449X"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YTHyH5eFeJAS",
        "gn4WD4IJeCac",
        "UugYZ-uNHv7L",
        "v86NFkFcQA3C"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "74a539bb1a30b12322b0673657a3362ba97f8c77e4e9b57fd18ae9f35996046a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}